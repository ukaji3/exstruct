# ExStruct Benchmark

This benchmark compares methods for answering questions about Excel documents using GPT-4o:

- exstruct
- openpyxl
- pdf (xlsx->pdf->text)
- html (xlsx->html->table text)
- image_vlm (xlsx->pdf->png -> GPT-4o vision)

## Requirements

- Python 3.11+
- LibreOffice (`soffice` in PATH)
- OPENAI_API_KEY in `.env`

## Setup

```bash
cd benchmark
cp .env.example .env
pip install -e ..  # install exstruct from repo root
pip install -e .
```

## Run

```bash
make all
```

## Reproducibility script (Windows PowerShell)

```powershell
.\scripts\reproduce.ps1
```

Options:

- `-Case` (default: `all`)
- `-Method` (default: `all`)
- `-Model` (default: `gpt-4o`)
- `-Temperature` (default: `0.0`)
- `-SkipAsk` (skip LLM calls; uses existing responses)

## Reproducibility script (macOS/Linux)

```bash
./scripts/reproduce.sh
```

If you see a permission error, run:

```bash
chmod +x ./scripts/reproduce.sh
```

Options:

- `--case` (default: `all`)
- `--method` (default: `all`)
- `--model` (default: `gpt-4o`)
- `--temperature` (default: `0.0`)
- `--skip-ask` (skip LLM calls; uses existing responses)

Outputs:

- outputs/extracted/\* : extracted context (text or images)
- outputs/prompts/\*.jsonl
- outputs/responses/\*.jsonl
- outputs/markdown/\*/\*.md
- outputs/markdown/responses/\*.jsonl
- outputs/results/results.csv
- outputs/results/report.md

## Markdown conversion (optional)

Generate Markdown from the latest JSON responses:

```bash
python -m bench.cli markdown --case all --method all
```

Markdown scores (`score_md`, `score_md_precision`) are only computed when
Markdown outputs exist under `outputs/markdown/responses/`.

If you want a deterministic renderer without LLM calls:

```bash
python -m bench.cli markdown --case all --method all --use-llm false
```

## RUB (lite)

RUB lite evaluates reconstruction utility using Markdown-only inputs.

Run Stage B tasks with the lite manifest:

```bash
python -m bench.cli rub-ask --task all --method all --manifest rub/manifest_lite.json
python -m bench.cli rub-eval --manifest rub/manifest_lite.json
python -m bench.cli rub-report
```

Outputs:

- outputs/rub/results/rub_results.csv
- outputs/rub/results/report.md

## Evaluation protocol (public)

To ensure reproducibility and fair comparison, follow these fixed settings:

- Model: gpt-4o (Responses API)
- Temperature: 0.0
- Prompt: fixed in `bench/llm/openai_client.py`
- Input contexts: generated by `bench.cli extract` using the same sources for all methods
- Normalization: optional normalized track uses `data/normalization_rules.json`
- Evaluation: `bench.cli eval` produces Exact, Normalized, Raw, and Markdown scores
- Report: `bench.cli report` generates `report.md` and per-case detailed reports

Recommended disclosure when publishing results:

- Model name + version, temperature, and date of run
- Full `normalization_rules.json` used for normalized scores
- Cost/token estimation method
- Any skipped cases and the reason (missing files, extraction failures)

## How to interpret results (public guide)

This benchmark reports four evaluation tracks to keep comparisons fair:

- Exact: strict string match with no normalization.
- Normalized: applies case-specific rules in `data/normalization_rules.json` to
  absorb formatting differences (aliases, split/composite labels).
- Raw: loose coverage/precision over flattened text tokens (schema-agnostic),
  intended to reflect raw data capture without penalizing minor label variations.
- Markdown: coverage/precision against canonical Markdown rendered from truth.

Recommended interpretation:

- Use **Exact** to compare end-to-end string fidelity (best for literal extraction).
- Use **Normalized** to compare **document understanding** across methods.
- Use **Raw** to compare how much ground-truth text is captured regardless of schema.
- Use **Markdown** to evaluate JSON-to-Markdown conversion quality.
- When methods disagree between tracks, favor Normalized for Excel-heavy layouts
  where labels are split/merged or phrased differently.
- Always cite both accuracy and cost metrics when presenting results publicly.

## Evaluation

The evaluator now writes four tracks:

- Exact: `score`, `score_ordered` (strict string match, current behavior)
- Normalized: `score_norm`, `score_norm_ordered` (applies case-specific rules)
- Raw: `score_raw`, `score_raw_precision` (loose coverage/precision)
- Markdown: `score_md`, `score_md_precision` (Markdown coverage/precision)

Normalization rules live in `data/normalization_rules.json` and are applied in
`bench.cli eval`. Publish these rules alongside the benchmark to keep the
normalized track transparent and reproducible.

## Notes:

- GPT-4o Responses API supports text and image inputs. See docs:
  - [https://platform.openai.com/docs/api-reference/responses](https://platform.openai.com/docs/api-reference/responses)
  - [https://platform.openai.com/docs/guides/images-vision](https://platform.openai.com/docs/guides/images-vision)
- Pricing for gpt-4o used in cost estimation:
  - https://platform.openai.com/docs/models/compare?model=gpt-4o
